# -*- coding: utf-8 -*-
"""Knowmap cross domain knowledge mapping using AI .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ccMeB2TUuFN8dchwAxHnAdQtZPYJZpXz
"""

!pip -q install streamlit pyngrok pandas spacy networkx pyvis sentence-transformers
!python -m spacy download en_core_web_sm

!pip install pyvis==0.2.1

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# # (This will contain all milestones: Authentication, Dataset, NLP Extraction, Graph, Feedback)
# # Iâ€™ll paste the full integrated code here in the next message to keep it neat and runnable.
#

!ls -lh app.py

# Commented out IPython magic to ensure Python compatibility.
#  %%writefile app.py
import streamlit as st
import pandas as pd
import spacy
import json
import os
from io import StringIO

st.set_page_config(page_title="KnowMap | Cross-Domain Knowledge Mapping", layout="wide")

# -------------------------------
# USER DATABASE
# -------------------------------
USER_DB = "users.json"
if not os.path.exists(USER_DB):
    with open(USER_DB, "w") as f:
        json.dump({}, f)

def load_users():
    with open(USER_DB, "r") as f:
        return json.load(f)

def save_users(data):
    with open(USER_DB, "w") as f:
        json.dump(data, f)

# -------------------------------
# ROBUST CSV LOADER
# -------------------------------
def load_dataframe(uploaded_file):
    """Auto-detect delimiter, handle missing headers, and name columns properly."""
    uploaded_file.seek(0)
    try:
        df = pd.read_csv(uploaded_file, sep=None, engine="python", header=0)
    except Exception:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, sep=None, engine="python", header=None)

    lower_cols = [str(c).lower() for c in df.columns]
    if not {"entity_1", "relation", "entity_2"}.issubset(set(lower_cols)):
        if df.shape[1] >= 9:
            df.columns = [
                "id","entity_1","relation","entity_2",
                "domain","country","start_year","end_year","notes"
            ]
        elif df.shape[1] >= 3:
            df.columns = ["entity_1","relation","entity_2"] + [
                f"col{i}" for i in range(df.shape[1]-3)
            ]
    return df

# -------------------------------
# HEADER
# -------------------------------
st.markdown(
    "<h1 style='text-align:center; color:#FF7F50;'>KnowMap: Cross-Domain Knowledge Mapping</h1>",
    unsafe_allow_html=True,
)
st.markdown("---")

tabs = st.tabs([
    "ðŸ  Welcome",
    "ðŸ” Authentication",
    "ðŸ“‚ Dataset Management",
    "ðŸ§  NLP Extraction",
    "ðŸŒ Semantic Search",
    "ðŸ“Š Admin Dashboard",
    "ðŸ’¬ Feedback",
])

c

# -------------------------------
# TAB 2: AUTHENTICATION
# -------------------------------
with tabs[1]:
    st.header("ðŸ” User Authentication (Milestone 1)")
    users = load_users()

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Login")
        email = st.text_input("Email", key="login_email")
        pw = st.text_input("Password", type="password", key="login_pw")
        if st.button("Login"):
            if email in users and users[email]["password"] == pw:
                st.session_state["logged_in"] = email
                st.success(f"Welcome back, {email}!")
            elif email in users:
                st.error("âŒ Wrong password.")
            else:
                st.warning("âš ï¸ User not registered.")

    with col2:
        st.subheader("Register")
        new_email = st.text_input("New Email", key="reg_email")
        new_pw = st.text_input("New Password", type="password", key="reg_pw")
        if st.button("Register"):
            if new_email in users:
                st.warning("âš ï¸ Email already exists.")
            elif not new_email or not new_pw:
                st.error("âŒ Enter valid email and password.")
            else:
                users[new_email] = {"password": new_pw}
                save_users(users)
                st.success("ðŸŽ‰ Registration successful!")

    if "logged_in" in st.session_state:
        st.info(f"Logged in as: {st.session_state['logged_in']}")

# -------------------------------
# TAB 3: DATASET MANAGEMENT
# -------------------------------
with tabs[2]:
    st.header("ðŸ“‚ Dataset Management (Milestone 2)")
    if "logged_in" not in st.session_state:
        st.warning("Please login first to access dataset tools.")
    else:
        upload_type = st.radio("Upload Method", ["CSV Upload", "Text Input"])

        if upload_type == "CSV Upload":
            uploaded_file = st.file_uploader("Upload CSV/TSV File", type=["csv","tsv","txt"])
            if uploaded_file:
                df = load_dataframe(uploaded_file)
                st.session_state["uploaded_df"] = df
                st.success("âœ… File uploaded successfully!")
                st.dataframe(df.head(10))

                # Extract and store triples automatically
                cols = {c.lower(): c for c in df.columns}
                s_col = cols.get("entity_1") or cols.get("subject")
                p_col = cols.get("relation") or cols.get("predicate") or cols.get("rel")
                o_col = cols.get("entity_2") or cols.get("object")
                if s_col and p_col and o_col:
                    triples = list(zip(df[s_col].astype(str), df[p_col].astype(str), df[o_col].astype(str)))
                    st.session_state["triples"] = triples
                    st.info(f"âœ… Found {len(triples)} triples â€” automatically saved for NLP & Graph tabs.")
                else:
                    st.warning("No triple columns detected. Use the NLP Extraction tab to generate triples.")
        else:
            text_input = st.text_area("Paste your dataset text here")
            if text_input:
                st.session_state["text_data"] = text_input
                st.success("âœ… Text data stored in memory!")

# -------------------------------
# TAB 4: NLP EXTRACTION
# -------------------------------
with tabs[3]:
    st.header("ðŸ§  NLP Extraction â€” Entity & Relation (Milestone 2)")

    if "logged_in" not in st.session_state:
        st.warning("âš ï¸ Please login first.")
    else:
        def extract_triples_from_text(text: str):
            """Extract minimal triples (S, P, O) even for single-sentence text."""
            try:
                nlp_local = spacy.load("en_core_web_sm")
            except Exception:
                st.error("spaCy model not found. Run: !python -m spacy download en_core_web_sm")
                return []
            doc = nlp_local(text)
            triples = []
            for sent in doc.sents:
                root = next((t for t in sent if t.dep_ == "ROOT"), None)
                if not root:
                    continue
                subj = next((w for w in root.lefts if "subj" in w.dep_), None)
                obj = next((w for w in root.rights if "obj" in w.dep_ or w.dep_ == "pobj"), None)
                if subj and obj:
                    triples.append((subj.text, root.lemma_, obj.text))
            if not triples and len(doc.ents) >= 2:
                triples = [(doc.ents[0].text, "related_to", doc.ents[1].text)]
            if not triples and len(doc) >= 3:
                tokens = [t.text for t in doc if not t.is_space]
                triples = [(tokens[0], tokens[1], tokens[2])]
            return triples

        triples_out = st.session_state.get("triples", None)

        if triples_out:
            st.success("âœ… Automatically extracted triples from uploaded CSV!")
            st.dataframe(pd.DataFrame(triples_out, columns=["Subject","Relation","Object"]))
        else:
            st.info("No triples found in dataset. You can enter text below to extract manually:")
            text_input = st.text_area("Enter text for extraction", height=150)
            if st.button("ðŸš€ Run NLP Extraction"):
                if not text_input.strip():
                    st.error("Please enter some text.")
                else:
                    triples_out = extract_triples_from_text(text_input)
                    st.session_state["triples"] = triples_out
                    st.dataframe(pd.DataFrame(triples_out, columns=["Subject","Relation","Object"]))
                    st.success("âœ… Extraction completed and saved for Knowledge Graph visualization.")

# -------------------------------
# TAB 5: SEMANTIC SEARCH + GRAPH (CLEAN & FIXED)
# -------------------------------
with tabs[4]:
    st.header("ðŸŒ Knowledge Graph Visualization & Semantic Search (Milestone 3)")

    import tempfile
    import networkx as nx
    from pyvis.network import Network

    triples = st.session_state.get("triples", [])
    if not triples:
        st.warning("No triples available. Upload CSV or run NLP first.")
    else:
        # --- Settings ---
        show_labels = st.checkbox("Show relation labels", value=False)
        top_k = st.number_input("Top-K matches", 1, 20, 5)

        # --- Build Graph ---
        G = nx.Graph()
        for s, p, o in triples:
            G.add_node(s)
            G.add_node(o)
            G.add_edge(s, o, relation=p)

        # --- Summary ---
        c1, c2, c3 = st.columns(3)
        c1.metric("Nodes", G.number_of_nodes())
        c2.metric("Edges", G.number_of_edges())
        avg_deg = round(sum(dict(G.degree()).values()) / max(1, G.number_of_nodes()), 2)
        c3.metric("Avg degree", avg_deg)
        st.markdown("---")

        # --- Semantic Search (cached) ---
        @st.cache_resource
        def load_st_model():
            from sentence_transformers import SentenceTransformer
            return SentenceTransformer("all-MiniLM-L6-v2")

        @st.cache_data
        def embed_sentences(sentences):
            import numpy as np
            model = load_st_model()
            return model.encode(sentences, convert_to_numpy=True)

        sentences = [f"{s} {p} {o}" for s, p, o in triples]
        query = st.text_input("ðŸ” Semantic Search (e.g., 'Einstein physics')")

        highlight_nodes, highlight_edges = set(), set()
        if query.strip():
            try:
                import numpy as np
                from sklearn.metrics.pairwise import cosine_similarity

                emb = embed_sentences(sentences)
                q = load_st_model().encode([query], convert_to_numpy=True)
                scores = cosine_similarity(q, emb)[0]
                idxs = np.argsort(scores)[::-1][:top_k]

                results = []
                for i in idxs:
                    s, p, o = triples[i]
                    results.append({"Subject": s, "Relation": p, "Object": o, "Score": float(scores[i])})
                    highlight_nodes.update([s, o])
                    highlight_edges.add((s, o, p))
                st.subheader("Results")
                st.dataframe(pd.DataFrame(results), use_container_width=True)
            except Exception as e:
                st.error(f"Semantic search failed: {e}")

        # --- Graph Display ---
        H = G.copy()
        deg = dict(H.degree())
        max_deg = max(deg.values()) if deg else 1
        def size_for(n): return 16 + int(22 * (deg.get(n,0) / max_deg))

        net = Network(height="680px", width="100%", bgcolor="#0e1117", font_color="white")
        net.barnes_hut(gravity=-40000, central_gravity=0.25, spring_length=260, spring_strength=0.02, damping=0.5)

        for n in H.nodes():
            color = "#FF6B6B" if n in highlight_nodes else "#7FB3FF"
            net.add_node(n, label=n, title=n, color=color, size=size_for(n))

        for u, v, data in H.edges(data=True):
            rel = data.get("relation","")
            is_hit = (u, v, rel) in highlight_edges or (v, u, rel) in highlight_edges
            edge_kwargs = dict(title=rel)
            if show_labels:
                edge_kwargs["label"] = rel
            if is_hit:
                edge_kwargs["width"] = 3
            net.add_edge(u, v, **edge_kwargs)

        # Try to set options (safe for mixed pyvis versions)
        try:
            net.set_options("""
{
  "nodes": {"borderWidth": 2, "shape": "dot", "font": {"size": 16}},
  "edges": {
    "font": {"size": 12, "background": "rgba(14,17,23,0.85)"},
    "color": {"inherit": false, "opacity": 0.8},
    "smooth": {"type": "continuous"}
  },
  "physics": {
    "barnesHut": {"gravitationalConstant": -40000, "springLength": 260},
    "stabilization": {"enabled": true, "iterations": 250}
  },
  "interaction": {"hover": true, "tooltipDelay": 120, "zoomView": true, "dragView": true}
}
""")
        except Exception:
            pass  # fall back to defaults if pyvis version differs

        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".html")
        net.write_html(tmp.name)
        with open(tmp.name, "r", encoding="utf-8") as f:
            html = f.read()
        st.components.v1.html(html, height=720, scrolling=True)
        os.unlink(tmp.name)

        st.markdown("**Triple Preview**")
        st.dataframe(pd.DataFrame(triples[:25], columns=["Subject","Relation","Object"]))


        # -------------------------------
# TAB 6: ADMIN DASHBOARD (Milestone 4)
# -------------------------------
with tabs[5]:
    import networkx as nx
    import json
    import datetime
    from io import BytesIO

    st.header("ðŸ›  Admin Dashboard â€” Graph & User Management (Milestone 4)")

    # --- Helpers & persistence ---
    ADMIN_LOG = "admin_log.json"
    BACKUP_DIR = "kg_backups"
    os.makedirs(BACKUP_DIR, exist_ok=True)

    def log_admin(action, detail=""):
        entry = {"ts": datetime.datetime.utcnow().isoformat() + "Z", "action": action, "detail": detail}
        logs = []
        if os.path.exists(ADMIN_LOG):
            try:
                with open(ADMIN_LOG, "r") as f:
                    logs = json.load(f)
            except Exception:
                logs = []
        logs.insert(0, entry)
        with open(ADMIN_LOG, "w") as f:
            json.dump(logs[:200], f, indent=2)

    def build_graph_from_session():
        triples = st.session_state.get("triples", [])
        G = nx.Graph()
        for s, p, o in triples:
            G.add_node(s); G.add_node(o)
            G.add_edge(s, o, relation=p)
        return G

    def triples_from_graph(G):
        triples = []
        for u, v, data in G.edges(data=True):
            triples.append((u, data.get("relation", ""), v))
        return triples

    # ensure we have a graph object available for admin actions
    G_admin = build_graph_from_session()
    st.subheader("Quick stats")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Nodes", G_admin.number_of_nodes())
    c2.metric("Edges", G_admin.number_of_edges())
    degrees = dict(G_admin.degree())
    top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:5]
    c3.metric("Top node degree", top_nodes[0][1] if top_nodes else 0)
    # top relation counts
    rels = {}
    for _,_,d in G_admin.edges(data=True):
        rels[d.get("relation","")] = rels.get(d.get("relation",""), 0) + 1
    top_rels = sorted(rels.items(), key=lambda x: x[1], reverse=True)[:3]
    c4.metric("Top relation (count)", f"{top_rels[0][0]} ({top_rels[0][1]})" if top_rels else "N/A")

    st.markdown("---")
    st.subheader("Manual Graph Controls")

    # --- Rename node ---
    col_a, col_b = st.columns([2,2])
    node_to_rename = col_a.text_input("Node to rename (exact)", key="admin_rename_from")
    node_new_name = col_b.text_input("New name", key="admin_rename_to")
    if st.button("Rename Node"):
        if not node_to_rename or not node_new_name:
            st.error("Provide both existing node name and new name.")
        elif node_to_rename not in G_admin:
            st.error("Node not found in current graph.")
        else:
            # backup
            ts = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            backup_path = os.path.join(BACKUP_DIR, f"backup_before_rename_{ts}.json")
            with open(backup_path, "w") as f: json.dump(triples_from_graph(G_admin), f)
            # rename
            nx.relabel_nodes(G_admin, {node_to_rename: node_new_name}, copy=False)
            st.session_state["triples"] = triples_from_graph(G_admin)
            log_admin("rename_node", f"{node_to_rename} -> {node_new_name}")
            st.success(f"Renamed '{node_to_rename}' to '{node_new_name}'. Backup: {backup_path}")

    # --- Merge nodes ---
    merge_a = st.text_input("Merge Node A (target)", key="admin_merge_a")
    merge_b = st.text_input("Merge Node B (source)", key="admin_merge_b")
    if st.button("Merge Nodes"):
        if not merge_a or not merge_b:
            st.error("Provide both node names.")
        elif merge_a not in G_admin or merge_b not in G_admin:
            st.error("One or both nodes not found.")
        else:
            ts = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            backup_path = os.path.join(BACKUP_DIR, f"backup_before_merge_{ts}.json")
            with open(backup_path, "w") as f: json.dump(triples_from_graph(G_admin), f)
            # move neighbors of B to A
            for nbr in list(G_admin.neighbors(merge_b)):
                if not G_admin.has_edge(merge_a, nbr) and nbr != merge_a:
                    G_admin.add_edge(merge_a, nbr, **G_admin.edges[merge_b, nbr])
            G_admin.remove_node(merge_b)
            st.session_state["triples"] = triples_from_graph(G_admin)
            log_admin("merge_nodes", f"{merge_b} -> {merge_a}")
            st.success(f"Merged '{merge_b}' into '{merge_a}'. Backup: {backup_path}")

    # --- Delete node ---
    node_delete = st.text_input("Delete node (exact)", key="admin_delete_node")
    if st.button("Delete Node"):
        if not node_delete:
            st.error("Provide node name to delete.")
        elif node_delete not in G_admin:
            st.error("Node not found.")
        else:
            ts = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            backup_path = os.path.join(BACKUP_DIR, f"backup_before_delete_{ts}.json")
            with open(backup_path, "w") as f: json.dump(triples_from_graph(G_admin), f)
            G_admin.remove_node(node_delete)
            st.session_state["triples"] = triples_from_graph(G_admin)
            log_admin("delete_node", node_delete)
            st.success(f"Deleted node '{node_delete}'. Backup: {backup_path}")

    st.markdown("---")
    st.subheader("Graph Import / Export")

    # Export GraphML
    if st.button("Export graph to GraphML"):
        path = os.path.join(BACKUP_DIR, f"knowledge_graph_{datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.graphml")
        nx.write_graphml(G_admin, path)
        with open(path, "rb") as f:
            st.download_button("Download GraphML", f, file_name=os.path.basename(path))
        log_admin("export_graphml", path)

    # Export JSON triples
    if st.button("Export triples as JSON"):
        data = triples_from_graph(G_admin)
        b = BytesIO(json.dumps(data, indent=2).encode("utf-8"))
        st.download_button("Download triples.json", b, file_name="triples.json")
        log_admin("export_json", "triples.json")

    # Upload a saved graph (GraphML or JSON)
    uploaded_graph = st.file_uploader("Upload GraphML or triples JSON to restore/merge", type=["graphml","json"])
    if uploaded_graph is not None:
        try:
            fname = uploaded_graph.name.lower()
            if fname.endswith(".graphml"):
                H = nx.read_graphml(uploaded_graph)
                # convert GraphML to our triples format and replace session graph
                new_triples = []
                for u, v, data in H.edges(data=True):
                    rel = data.get("relation", "") or data.get("label", "") or ""
                    new_triples.append((u, rel, v))
                st.session_state["triples"] = new_triples
                st.success("Loaded GraphML into session (triples replaced).")
                log_admin("import_graphml", uploaded_graph.name)
            else:
                # assume JSON triples list
                payload = json.load(uploaded_graph)
                # validate
                if isinstance(payload, list) and all(len(x) >= 3 for x in payload):
                    st.session_state["triples"] = [(str(x[0]), str(x[1]), str(x[2])) for x in payload]
                    st.success("Loaded triples JSON into session.")
                    log_admin("import_json", uploaded_graph.name)
                else:
                    st.error("JSON file not recognized as triples list.")
        except Exception as e:
            st.error(f"Failed to load graph: {e}")

    st.markdown("---")
    st.subheader("User Management")

    # list users and allow deletion
    users = load_users()
    st.write(f"Total users: {len(users)}")
    if users:
        for u in list(users.keys()):
            col1, col2 = st.columns([4,1])
            col1.write(u)
            if col2.button(f"Delete {u}", key=f"deluser_{u}"):
                users.pop(u, None)
                save_users(users)
                log_admin("delete_user", u)
                st.success(f"Deleted user {u}")
                st.experimental_rerun()

    st.markdown("---")
    st.subheader("Activity Log (latest)")
    if os.path.exists(ADMIN_LOG):
        try:
            with open(ADMIN_LOG, "r") as f:
                logs = json.load(f)
            st.dataframe(pd.DataFrame(logs).head(50))
        except Exception:
            st.write("Log read error.")
    else:
        st.info("No admin activity yet.")

    st.markdown("---")
    st.write("Tip: Always export or backup before destructive admin actions.")


    # -------------------------------
# TAB 7: FEEDBACK (Milestone 5)
# -------------------------------
with tabs[6]:
    st.header("ðŸ’¬ User Feedback & Peer Testing (Milestone 5)")
    st.write("Help us improve KnowMap by sharing your feedback!")

    rating = st.slider("Rate your overall experience (1 = Poor, 5 = Excellent)", 1, 5, 4)
    graph_relevance = st.slider("Graph Relevance (1â€“5)", 1, 5, 4)
    comments = st.text_area("Your comments:", placeholder="What did you like or what can be improved?")

    if st.button("Submit Feedback"):
        feedback = {
            "rating": rating,
            "graph_relevance": graph_relevance,
            "comments": comments
        }
        with open("feedback.json", "a") as f:
            f.write(json.dumps(feedback) + "\n")
        st.success("âœ… Thank you for your feedback!")

!ls -lh app.py

from pyngrok import ngrok

ngrok.set_auth_token("34nHfPukbCwbGSgNsVbos3hQ7K3_2JvvHrTeYKjeaGRx8iiq5")

port = 8501
# Ensure the port is free
!fuser -k 8501/tcp || true

# Launch Streamlit in background
get_ipython().system_raw(f"streamlit run app.py --server.port {port} --server.headless true &")

# Reuse an existing tunnel if any; otherwise create one
tunnels = ngrok.get_tunnels()
if tunnels:
    public_url = tunnels[0].public_url
else:
    public_url = ngrok.connect(addr=port, proto="http").public_url

print("ðŸ”— App URL:", public_url)

from google.colab import auth
auth.authenticate_user()

!git config --global user.email "alok58484@gmail.com"
!git config --global user.name "CODEBRAKERBOYY"